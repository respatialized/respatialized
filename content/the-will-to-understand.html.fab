✳= (ns respatialized.writing.will-to-understand) 🔚

✳ (def metadata {:title "The Will To Understand"})  🔚

✳= (list [:h1 (:title metadata)]
       [:h2 "Why I am not interested in using AI in my intellectual and creative practice"]) 🔚
       
I took up drawing just as Silicon Valley told me my skill would be rendered irrelevant. I entered the professional field of machine learning just as I was told deep learning would revolutionize the scientific process. In both cases I have not allowed my own ambitions to be cowed by the pageantry surrounding the tech industry's shiny new toy.

In this post I offer a personal view as to why two major crazes in the history of machine learning -  deep learning and generative AI - do not really address the types of questions and work that drew me to programming and art, respectively, in the first place.

✳= [:h3 "Learning to see"] 🔚


✳= [:figure [:img {:src "/media/feynman-blackboard.webp"}]
  [:figcaption "Richard Feynman's blackboard at the time of his death, reading \"what I cannot create, I do not understand.\""]] 🔚
  
I learn nothing from a vague prompt that I poke with words until it looks even more vaguely satisfactory. By trying, and failing, to see the masses and edges of a tree, I learn much more. I learn how to draw a consistent line, how to simplify an object based on its distance to me and the light falling on it, 

My knowledge and ability grows, unmediated by a company's need to surveil my own creative "process" to extract information from it that they will attempt to use to replace me. I learn from the knowledge of others in a genuinely social way - by observing a teacher and having my own work observed - not through the ersatz sociality of an agglomeration of mediocrity vacuumed up from the internet.


✳= [:h3 "(YA)MLops"] 🔚
There's also the mundane aspects. Working with opaque ML systems is not enjoyable. The initial thrill of feeling like you've cracked the problem or gotten an acceptable level of predictable accuracy quickly gives way to bureaucratic tedium. You have a very brittle process that requires constant monitoring, tinkering, and instrumentation to make sure you're not facing performance regressions. Most of this monitoring isn't well-supported within the actual programming context, so you have to reach outside your programming language of choice for a half-baked wrapper atop a database that lets you remember what your parameters are. You end up declare all the components of your workflow in a plaintext environment where composition is impossible, consistency is not enforced, and you have no query capabilities.

You don't have to take my word for it. Several researchers at Google were uniquely well-suited to identify the difficulties in building and maintaining ML systems that perform useful work in the real world, and they described them quite well nearly a decade ago in the paper "Machine Learning: The High-Interest Credit Card of Technical Debt."  The paper identifies several sources of debt: data debt, pipeline debt, and configuration debt that are likely to impose very high maintenance burdens on anyone working with ML systems. If the proposed design of LLM-based systems is any indication, the interest rate on the LLM credit card is substantially higher than the ML card.

Anyone thinking of ascending the mountain of madness specified in Andressen Horowitz's work of speculative fiction ✳= [:a {:href "https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/"} "\"Emerging Architectures for LLMs\""] 🔚 should read through this paper several times and ask themselves if they are prepared for the journey, and what they expect to find at its conclusion.

✳= [:figure [:img {:src "/media/a16z-llm-madness.webp"}]
  [:figcaption "(not pictured: how the system interfaces with any part of your business that adds value. good luck!)" ]] 🔚
  


One of the reasons I enjoy programming is because, when I do it well, I can directly map my conceptual representations of a problem to components in the code, thus achieving an organizational structure that suits both myself as the maintainer of the system and the machine the system runs on. I like to half-jokingly refer to this as "applied metaphysics." With prompt-based language models as an important component of software, such a conceptual and practical separation of concerns is impossible. Everything is just text.

✳= [:figure
  [:blockquote
   [:p
    "Every integration, so far, of a language model with a larger production system involved jamming the control prompt, provided by the developer, and the input, provided by the end-user, together and schlepping it over to the language model that interprets it as a single text."]
   [:p "The control prompt usually included language that tells the model not to listen to control statements in the input, but because it’s all input into the model as one big slop, there’s nothing really to prevent an adversarial end-user from finding ways to countermand the commands in the developer portion of the prompt."]]
  [:figcaption [:a {:href "https://softwarecrisis.dev/letters/prompts-are-not-fit-for-purpose/"} "Prompts are unsafe, and that means language models are not fit for purpose"]]] 🔚
  


Bjarnason correctly identifies this as a major security issue, because attackers can exploit parts of the prompts as an arbitrary "control surface," but I believe it is much broader than that. The fact that all signals in the LLM component of the system are just strings means that there can be no "separation of concerns", whereby only the information necessary to perform a specific task is conveyed from one subsystem to another. If the program uses a language model to perform multiple different tasks, they could easily mix with one another. Imagine injecting non-determinism and the potential for noise into every stack frame executed by your program and you can get an idea of what it might be like to program with LLMs for an extended period of time.

The difficulty was anticipated by the authors of the ML paper:

✳=  [:blockquote
   "From a high level perspective, a machine learning package is a tool for mixing data sources together. That is, machine learning models are machines for creating entanglement and making the isolation of improvements effectively impossible.

To make this concrete, imagine we have a system that uses features x1, ...xn in a model. If we change the input distribution of values in x1, the importance, weights, or use of the remaining n − 1 features may all change—this is true whether the model is retrained fully in a batch style or allowed to adapt in an online fashion. Adding a new feature xn+1 can cause similar changes, as can removing any feature xj. No inputs are ever really independent. We refer to this here as the CACE principle: Changing Anything Changes Everything.

The net result of such changes is that prediction behavior may alter, either subtly or dramatically, on various slices of the distribution. The same principle applies to hyper-parameters. Changes in regularization strength, learning settings, sampling methods in training, convergence thresholds, and essentially every other possible tweak can have similarly wide ranging effects."] 🔚

I may have a strong preference for dynamically-typed languages, but I still appreciate that I get an exception when I try to increment a string. 

The gigantic apparatus described by the a16z chart is an implicit admission that language models, as statistical text generators, intrinsically ✳=[:em "cannot be programmed."]🔚 The type of control, monitoring, and separation of concerns that programmers like myself are accustomed to must be bolted on to them after the fact.  I have no interest in a tool that requires a devops and cloud software budget of a million dollars a year to operate safely. 

✳= [:figure [:blockquote "If a system is to serve the creative spirit, it must be entirely comprehensible to a single individual. The point here is that the human potential manifests itself in individuals. To realize this potential, we must provide a medium that can be mastered by a single individual. Any barrier that exists between the user and some part of the system will eventually be a barrier to creative expression."]
  [:figcaption "Dan Ingalls"]]  🔚


✳= [:h3 "Intellectual strip mining"] 🔚 

In understanding the current boom, I think it is worth examining a common cliche with a bit more of an eye to its historical context. If that chart from a16z is any indication, in the AI gold rush, a lot more people are going to get rich selling the shovels of "LLMops" than by panning for gold in the streams of grammatical nonsense generated by language models.


Usually the historical analogy stops there, but it is worth remembering that when the naturally occurring rivers in the northern California landscape proved insufficient to the task of dissolving enough soil to find the gold sought by prospectors, they turned to more destructive means.

✳= [:figure
  [:img {:src "/media/1093px-Henry_Sandham_-_The_Monitor.jpeg"}]] 🔚
  
Whole hillsides were ripped away by ✳= [:a {:href "https://www.kcet.org/shows/earth-focus/mercury-in-our-waters-the-10-000-year-legacy-of-californias-gold-rush"} "hydraulic mining"] 🔚 in the search for gold, leading to toxic runoff that persists to this day, the destruction an incredibly rich ecosystem, and mass death for the tribal communities that lived there before the gold rush.

An electronic equivalent to ecosystem collapse may occur as the output of language and image generation models begins to pollute the environment being drawn from for the purposes of training data. 

✳= [:figure
  [:img {:src "/media/curse-of-recursion.jpg"}]] 🔚

Web-based intellectual ecosystems have been undergoing collapse for some time prior to the widespread use of large language models like ChatGPT.

Trivially-deployable bots overwhelm the human users of Q&A sites (https://github.com/RanjithJames/Reddit---Quora-question-BOT)
Estimates suggest about 1 in 3 website visitors is an attack bot (https://www.theatlantic.com/technology/archive/2017/01/bots-bots-bots/515043/)
Content mills drown out useful information in search results (https://www.technologyreview.com/2010/07/26/26327/the-search-engine-backlash-against-content-mills/)

All of those articles were written well before ChatGPT came on to the scene; language models did not create this trend, but they will certainly accelerate it. Efforts to thwart wholesale data harvesting on Twitter have been met with equally heavy-handed responses like blocking rate-limiting and requiring logins merely to view tweets - both of which attack the very foundations of the open internet. Those who thought that their Twitter feed would be a simple and public way of sharing their updates with the world have now found their reach limited to other Twitter users, and those like myself who have resisted the shrill siren song of the bird website are frozen out. OpenAI, as the first mover, very likely already has their hands on a large historical archive of Twitter data; it is everyone else who must pay the price.


✳= [:blockquote "LLMs are an innovation that reprivatizes public space. It creatively destroys the organization of information that the internet had once facilitated, back before it had come to depend on the environment of manipulation and deceit that characterizes advertising. “AI” indexes information without requiring social interaction, without any people organizing it with the needs of other people in mind. AI gets around having to have a “useful internet” that structures ways for people to develop trust in one another and the information they provide on an ongoing basis, beyond the kinds of branding and inculcation developed to support consumerism. It presents information with apparent directness, without the social mediation that contextualizes it; it organizes information as though it is all equally valid, no matter what purpose it originally served. It makes it seem as though all information has never been any better and any worse than advertising."] 🔚

The enclosure and strip-mining of the intellectual commons will lay waste to whatever is left of the complex intellectual ecosystem on the web that contains genuine knowledge to try and mine it for answers to queries that seem barely plausible enough to get past users' bullshit detectors. The  ✳= [:a {:href "https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web"} "jpeg blurs"] 🔚 into meaningless noise as the the accumulated errors of the model overwhelm the signal of the real world. 

Plastic may be safe to eat from, but it will still cause harm to your body and the ecosystem that supports it in the long term. I am inclined to think that the near-term utility of LLMs will be greatly outweighed by toxic effects further down the line.

✳= [:h3 "The irreducible labor of thinking"] 🔚

As far as ChatGPT and its utility for my own writing, Bjarnason said it far better than I could:
✳= [:blockquote "One aspect of writing that tends to get lost in all the discourse is that writing is thinking. The process of putting your thoughts into words is a form of reasoning that clarifies and condenses those thoughts. Writing is how I discover what matters to me. Writing is where I find out what I think.

This is why the first draft is often the hardest and why everybody dreads it. That’s the part where you dive into yourself and dredge up the truth itself.

Of course everybody wants to skip that."] 🔚

Unlike a meme that has become unfortunately fashionable among my political allies, I do dream of labor. I dream of the labor that I think is worth doing, and that I enjoy: the labor of making sense of the world, of acting within it to make it better, of trying, and failing, and trying again, to put the line in a place that correctly represents human anatomy, of participating in ecological remediation that helps us prepare for the climate of the next century, of building tools that can help scientists and workers reduce our dependencies on resources that require destructive extraction from beneath the earth.

Physical labor, as long as we have the organization necessary to prevent it from becoming back-breaking, can make us stronger in body. Intellectual labor, as long as we have the organization necessary to prevent it from becoming a mentally deadening bullshit job, can make us stronger in mind. I dream, as someone else once did, of a world where participation on ✳= [:em "both"] 🔚 kinds of labor creates an "association, in which the free development of each is the condition for the free development of all." 

A world "where nobody has one exclusive sphere of activity but each can become accomplished in any branch he wishes, society regulates the general production and thus makes it possible for me to do one thing today and another tomorrow, to hunt in the morning, fish in the afternoon, rear cattle in the evening, criticise after dinner, just as I have a mind, without ever becoming hunter, fisherman, herdsman or critic."

No language model will save me from the intellectual and political labor required to bring us closer to that world. 
