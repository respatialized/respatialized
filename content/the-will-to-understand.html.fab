âœ³= (ns respatialized.writing.will-to-understand) ðŸ”š

âœ³ (def metadata {:title "The Will To Understand"})  ðŸ”š

âœ³= (list [:h1 (:title metadata)]
       [:h2 "Why I am not interested in using AI in my intellectual and creative practice"]) ðŸ”š
       
I took up drawing just as Silicon Valley told me my skill would be rendered irrelevant. I entered the professional field of machine learning just as I was told deep learning would revolutionize the scientific process. In both cases I have not allowed my own ambitions to be cowed by the pageantry surrounding the tech industry's shiny new toy.

In this post I offer a personal view as to why two major crazes in the history of machine learning -  deep learning and generative AI - do not really address the types of questions and work that drew me to programming and art, respectively, in the first place.

âœ³= [:h3 "Learning to see"] ðŸ”š


âœ³= [:figure [:img {:src "/media/feynman-blackboard.webp"}]
  [:figcaption "Richard Feynman's blackboard at the time of his death, reading \"what I cannot create, I do not understand.\""]] ðŸ”š
  
I learn nothing from a vague prompt that I poke with words until it looks even more vaguely satisfactory. By trying, and failing, to see the masses and edges of a tree, I learn much more. I learn how to draw a consistent line, how to simplify an object based on its distance to me and the light falling on it, 

My knowledge and ability grows, unmediated by a company's need to surveil my own creative "process" to extract information from it that they will attempt to use to replace me. I learn from the knowledge of others in a genuinely social way - by observing a teacher and having my own work observed - not through the ersatz sociality of an agglomeration of mediocrity vacuumed up from the internet.


âœ³= [:h3 "(YA)MLops"] ðŸ”š
There's also the mundane aspects. Working with opaque ML systems is not enjoyable. The initial thrill of feeling like you've cracked the problem or gotten an acceptable level of predictable accuracy quickly gives way to bureaucratic tedium. You have a very brittle process that requires constant monitoring, tinkering, and instrumentation to make sure you're not facing performance regressions. Most of this monitoring isn't well-supported within the actual programming context, so you have to reach outside your programming language of choice for a half-baked wrapper atop a database that lets you remember what your parameters are. You end up declare all the components of your workflow in a plaintext environment where composition is impossible, consistency is not enforced, and you have no query capabilities.

You don't have to take my word for it. Several researchers at Google were uniquely well-suited to identify the difficulties in building and maintaining ML systems that perform useful work in the real world, and they described them quite well nearly a decade ago in the paper "Machine Learning: The High-Interest Credit Card of Technical Debt."  The paper identifies several sources of debt: data debt, pipeline debt, and configuration debt that are likely to impose very high maintenance burdens on anyone working with ML systems, and if the proposed design of LLM-based systems is any indication, the interest rate on the LLM credit card is substantially higher than the ML card.

Anyone thinking of ascending the mountain of madness specified in Andressen Horowitz's work of speculative fiction âœ³= [:a {:href "https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/"} "\"Emerging Architectures for LLMs\""] ðŸ”š should read through this paper several times and ask themselves if they are prepared for the journey, and what they expect to find at its conclusion.

âœ³= [:figure [:img {:src "/media/a16z-llm-madness.webp"}]
  [:figcaption "(not pictured: how the system interfaces with any part of your business that adds value. good luck!)" ]] ðŸ”š
  
One of the reasons I enjoy programming is because, when I do it well, I can directly map my conceptual representations of a problem to components in the code, thus achieving an organizational structure that suits both myself as the maintainer of the system and the machine the system runs on. I like to half-jokingly refer to this as "applied metaphysics." With prompt-based language models as an important component of software, such a conceptual and practical separation of concerns is impossible. 

âœ³= [:figure
  [:blockquote
   [:p
    "Every integration, so far, of a language model with a larger production system involved jamming the control prompt, provided by the developer, and the input, provided by the end-user, together and schlepping it over to the language model that interprets it as a single text."]
   [:p "The control prompt usually included language that tells the model not to listen to control statements in the input, but because itâ€™s all input into the model as one big slop, thereâ€™s nothing really to prevent an adversarial end-user from finding ways to countermand the commands in the developer portion of the prompt."]]
  [:figcaption [:a {:href "https://softwarecrisis.dev/letters/prompts-are-not-fit-for-purpose/"} "Prompts are unsafe, and that means language models are not fit for purpose"]]] ðŸ”š
  


Bjarnason correctly identifies this as a major security issue, because attackers can exploit parts of the prompts as an arbitrary "control surface," but I believe it is much broader than that. The fact that all signals in the LLM component of the system are just strings means that there can be no "separation of concerns", whereby only the information necessary to perform a specific task is conveyed from one subsystem to another. If the program uses a language model to perform multiple different tasks, they could easily mix with one another. Imagine injecting non-determinism and the potential for noise into every stack frame executed by your program and you can get an idea of what it might be like to program with LLMs for an extended period of time.

The difficulty was anticipated by the authors of the ML paper:

âœ³=  [:blockquote
   "From a high level perspective, a machine learning package is a tool for mixing data sources together. That is, machine learning models are machines for creating entanglement and making the isolation of improvements effectively impossible.

To make this concrete, imagine we have a system that uses features x1, ...xn in a model. If we change the input distribution of values in x1, the importance, weights, or use of the remaining n âˆ’ 1 features may all changeâ€”this is true whether the model is retrained fully in a batch style or allowed to adapt in an online fashion. Adding a new feature xn+1 can cause similar changes, as can removing any feature xj. No inputs are ever really independent. We refer to this here as the CACE principle: Changing Anything Changes Everything.

The net result of such changes is that prediction behavior may alter, either subtly or dramatically, on various slices of the distribution. The same principle applies to hyper-parameters. Changes in regularization strength, learning settings, sampling methods in training, convergence thresholds, and essentially every other possible tweak can have similarly wide ranging effects."] ðŸ”š

I may have a strong preference for dynamically-typed languages, but I still appreciate that I get an exception when I try to increment a string. 


âœ³= [:h3 "The irreducible labor of thinking"] ðŸ”š

As far as ChatGPT and its utility for my own writing, Bjarnason said it far better than I could:
âœ³= [:blockquote "One aspect of writing that tends to get lost in all the discourse is that writing is thinking. The process of putting your thoughts into words is a form of reasoning that clarifies and condenses those thoughts. Writing is how I discover what matters to me. Writing is where I find out what I think.

This is why the first draft is often the hardest and why everybody dreads it. Thatâ€™s the part where you dive into yourself and dredge up the truth itself.

Of course everybody wants to skip that."] ðŸ”š

Unlike a meme that has become unfortunately fashionable among my political allies, I do dream of labor. I dream of the labor that I think is worth doing, and that I enjoy: the labor of making sense of the world, of acting within it to make it better, of trying, and failing, and trying again, to put the line in a place that correctly represents human anatomy, of participating in ecological remediation that helps us prepare for the climate of the next century, of building tools that can help scientists and workers reduce our dependencies on resources that require destructive extraction from beneath the earth.

Physical labor, as long as we have the organization necessary to prevent it from becoming back-breaking, can make us stronger in body. Intellectual labor, as long as we have the organization necessary to prevent it from becoming a mentally deadening bullshit job, can make us stronger in mind. I dream, as someone else once did, of a world where participation on âœ³= [:em "both"] ðŸ”š kinds of labor creates an "association, in which the free development of each is the condition for the free development of all." 

A world "where nobody has one exclusive sphere of activity but each can become accomplished in any branch he wishes, society regulates the general production and thus makes it possible for me to do one thing today and another tomorrow, to hunt in the morning, fish in the afternoon, rear cattle in the evening, criticise after dinner, just as I have a mind, without ever becoming hunter, fisherman, herdsman or critic."

No language model will save me from the intellectual and political labor required to bring us closer to that world. 
