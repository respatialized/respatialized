
âœ³ (ns respatialized.writing.scholastics-and-oracles
  (:require 
   [garden.core :refer [style css]]
   [garden.stylesheet :refer [at-import]]
   [site.fabricate.prototype.page :refer :all])) ðŸ”š
   



âœ³(def metadata {:title "Computational scholastics and programmable oracles"
               :summary ""}) ðŸ”š
               


âœ³= [:h1 (:title metadata)]ðŸ”š


âœ³= [:h2 "On the uselessness of LLM critique"]ðŸ”š

The thick smell of incense makes you feel a little light-headed. The oracle seems to speak with one voice, but vague shapes behind the veil suggest more than one person is behind it. The dim candlelight makes it difficult to tell. 

When you leave, you will remember only the prophecies that seemed true and forget what seemed false. Someone you know asked the oracle the same question you did and got a completely different response, but you want to believe it's because the oracle sees further then you.

Does this seem like a mystification? If it is, it is because OpenAI now deliberately mystifies the operation of their models. The comparison made by Jathan Sadowski of This Machine Kills turned out to be quite prescient, because the "oracle" framing has been embraced not just by critics like him, but also by people who embrace more of the creative and intellectual potential of LLMs:

âœ³=[:blockquote "LLMs when pushed to extremes will flood the zone with competing stories, causing all narrative sensemaking to break down into fever dream."] ðŸ”š

âœ³= [:figure
  [:blockquote
   "Given both the competitive landscape and the safety implications of large-scale models like GPT-4, this report contains no further details about the architecture (including model size), hardware, training compute, dataset construction, training method, or similar."]
  ]ðŸ”š
  
OpenAI has embraced the AI safety critique precisely to the extent necessary to shield their rituals from scrutiny and commercial imitation, and not a nanometer more. It is not an exaggeration to say they are now engaged in a wholly pseudoscientific enterprise, publishing "data" and "results" that no one can possibly replicate.

Nevertheless, the priestly caste around the oracle makes prophetic assertions intended to be taken by the public at face value. ChatGPT doesn't just predict words in a sequence: it is "reasoning." Its understanding of meaning provides a foundation so solid that we will soon build "semantic towers" âœ³=[:s "of Babel"] ðŸ”š atop it, formalizing away all of the vagueness inherent in ordinary language. The pageantry around the oracle sometimes suffices to get the neophytes of the AI cult to revise their theories of "truth" to entirely match up with the most fanciful interpretation of the latest impressive demo, which they rationalize to themselves by using the pseudo-Bayesian terminology of "updating their priors."

The force of whatever critique I might make matters little. We can try to identify shortcomings in how ChatGPT works, point out dubious epistemic and metaphysical assumptions underlying people's understanding of what it does, or use it as a programmable vibes-based text generator, but our ability to truly understand is the same in any case. The model doesn't yield reliably deterministic output. Internal mechanisms are now trade-secret information of a commercial venture bootstrapped out of a clever accounting scheme applied to 501(c)3 charitable contributions. Boosters and critics alike just perform post-hoc sensemaking about what the model does on the basis of the limited window we see.  All of us now, save for the lucky few researchers deemed worthy of OpenAI's initiation rites, are like the supplicants seeking insight and predictions. 

As much as I might desire to argue against overblown claims, I now fear that critique of these models reinforces the hype around them. And as someone who questions whether they should be built and introduced âœ³=[:em "at all"] ðŸ”š, I do not think critics should be engaged in the enterprise of generating ideas that could be used to "fix" them.

I do, however, remain wholly unconvinced that text sequence prediction models can simply "scale" their way to a generalized form of intelligence. 