‚ú≥ (ns respatialized.writing.semantic-overfitting) üîö


‚ú≥ (def metadata {:title "Semantic Overfit"}) üîö


‚ú≥= [:h1 (:title metadata)] üîö

There's a hypothesis circulating in meme form about large language models: that they are "spontaneously" developing new capabilities. Stephen Ornes has a good overview of these perspectives in ‚ú≥= [:a {:href "https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316/"} "an article for " [:em "Quanta"] " magazine."] üîö

You can trace this from the GPT-3 paper itself, which introduces the idea of "few-shot" learning to a broad audience, to a paper claiming that LLMs are developing theory of mind as a "byproduct" of token sequence prediction, to a now-pervasive hope (or fear) that language models don't need to have a model for the world or truth engineered into them because one will just arise with a sufficient number of parameters, a hypothesis commonly known as 'scaling maximalism'.

I think the examples discussed don't reveal the model's knowledge as much as the researchers' ignorance of language in general, and of the linguistic dataset used to train LLMs in particular.

‚ú≥= [:h2 "Mimicking Carefulness"] üîö

One of the capacities labeled "spontaneous" is the so-called "chain of thought" capabilities in LLMs.

‚ú≥=[:blockquote "a model prompted to explain itself (a capacity called chain-of-thought reasoning) could correctly solve a math word problem, while the same model without that prompt could not."] üîö

It is important to remember that the corpus of text contains both documents where a writer explains their reasoning, and ones where the writer just makes assertions. These models will learn from and attempt to replicate context-specific details from both. "More careful writers that explain themselves are correct more of the time" is almost a trivial observation. "Language model explains itself" is earth-shattering news. So which might it be? The article quotes Ellie Pavlick, a linguistics researcher, who says:

‚ú≥=[:blockquote
"‚ÄúThat‚Äôs what we‚Äôre all hoping is the case, that there‚Äôs some fundamental shift that happens when models are scaled up.‚Äù

The other, less sensational possibility, she said, is that what appears to be emergent may instead be the culmination of an internal, statistics-driven process that works through chain-of-thought-type reasoning. Large LLMs may simply be learning heuristics that are out of reach for those with fewer parameters or lower-quality data."] üîö

I think phrases like "explain yourself" or "describe how you got to this result" are the exact kinds of heuristics that could cause a model to weight text written by more careful writers more highly, and therefore more likely to replicate the correctly stated facts in the original texts.
‚ú≥= [:h2 "Local Maxima"] üîö

The story goes like: the model shows linear improvement (on the tasks envisioned by its creators) as scaling increases, but then when turned loose on the internet, people begin supplying all kinds of examples. Which of those are "spontaneous breakthroughs" and which of those are just "things we didn't think to ask before?" Some of these tasks may not have been specified in a problem statement that researchers used to describe the problem, but they are identified just fine by the objective function of "predict next token." But those things were present in the data all along, even if the creators didn't bother to look for them. 

As for sudden increases in performance, I personally do not find it surprising at all: the model was in a local maximum before, and increasing the number of parameters, or, potentially, just reinitializing the same model with the same data and the same number of parameters but with different randomization applied to its starting point, caused it to break out of that local maximum. Once it did so, it could identify more statistical regularities within the texts that its previous local maximum prevented it from identifying or retaining in its compressed copy of the data.

Stephen Wolfram has made some extremely grandiose predictions about the application of large language models

Imagine if we had trained a language model on examples of scientific writing when phlogiston theory or the idea of "spontaneous generation" was still in vogue. Would one expect a LLM deliver any novel scientific insights under such conditions by giving us an artificially precise definition of "phlogiston?" 

But we're so much smarter now, right? We couldn't possibly be stuck in a local scientific maximum requiring a paradigm shift to get out of, right? (oh right, I guess the paradigm shift will itself be supplied by the LLM, so I can just make the computer do my thinking for me)

Even if we "pop the hood" on the language model and determine that one specific 32x16,777,216 matrix of weights and biases corresponds to the term "car", what have we learned about cars? I wouldn't expect to learn anything about cars from the specific neurons or regions of the brain that light up when people are asked questions about cars. I might learn something about the brain - and so too we might learn something about the ‚ú≥=[:em "model"] üîö from that exercise. But trying to infer facts about the world from facts about the model is a category error.

So I think Wolfram's whole project is mistaken, and building it on the non-deterministic sand foundations of LLMs is not going to deliver the insights he thinks it will. It is telling that he cannot offer a more formal statement of ‚ú≥=[:em "how"] üîö LLMs will get around the vagueness of language than a weak analogy to formal logic.

This analogy is completely mistaken. First-order logic became a successful paradigm because you can do higher-order logic to ‚ú≥=[:em "prove"] üîö that the correct sets of premises never generate an incorrect conclusion. What proof could possibly be done that our 32x16,777,216 "car" matrix represents the concept faithfully? It would depend on facts about cars, not on facts about that matrix or its releationship with other matrices produced by the same model. Would we even get the same matrix for the term "car" if we re-trained the model on the same data and the same parameters but with randomization applied to the starting data? If we don't, then how can we possibly rely on this technique to engineer vagueness out of natural language?

‚ú≥=[:aside [:em [:a {:href "http://arayo.scripts.mit.edu/home/portfolio-archive/vague-representation/"} "also, vagueness is a feature, not a bug of language"]]] üîö

‚ú≥= [:h2 "Heuristic and Stereotype"] üîö
Another limitation of "scaling maximalism" is that the behavior of a model cannot be predictably guided as the number of parameters increases - sometimes with perverse results.

‚ú≥=[:blockquote "‚ÄúCertain harmful behaviors kind of come up abruptly in some models,‚Äù Ganguli said. He points to a recent analysis of LLMs, known as the BBQ benchmark, which showed that social bias emerges with enormous numbers of parameters. ‚ÄúLarger models abruptly become more biased.‚Äù"] üîö

The 'spontaneous' re-emergence of bias as a model is scaled up makes perfect sense when you consider the the paradigm used to condition LLMs like GPT-3. Known as "reinforcement learning with human feedback", it involves providing a model with human-labeled examples of truly horrific things and applying penalty scores to any learned associations with those things in the model's weights and outputs. These examples are intended to apply the same context-specific weighting mechanism used by the model in an overall capacity to the specific problem of generalizing from individual examples to broader patterns of harmful language.

I think it is relatively uncontroversial to note that the curated examples cannot possibly capture the full range of human bigotry that they hope the models avoid. Bigotry and bias can show up in extremely subtle ways that are not obvious, even to a cautious observer. And that's even assuming people agree on what the bias consists of, to say nothing at all of the fact that "bias" is itself a politically contested term.

This example, I think, provides additional compelling evidence for Pavlick's hypothesis that the LLM is just picking up more heuristics as the number of parameters increases. The model, through feedback, develops an imperfect heuristic for avoiding bias, but then identifies a way around that heuristic when it is given the ability to capture and store even more context and is exposed to even more horrific material. Whatever stereotypes it "unlearned" are replaced by subtler ones, not foreseen by the model's creators or the people tasked with helping the model avoid saying harmful things.

The model, when provided with prompts (I dislike and try to avoid the uncritical use of anthropomorphizing terms like "told"), containing instructions about avoiding stereotypes and bias, displays a reduction in bias. I do not find this revelatory or surprising. The prompt guides the model away from examples written by uncritical or biased writers and towards examples written by more careful writers, and phrases like "avoid stereotypes" can indeed serve as useful heuristics for fragments of writing about implicit bias. Once again, it can be made to mimic carefulness, but the carefulness is still supplied by examples in the training data.

The stochastic parrot is reminding us of things we've forgot we have said. That doesn't mean it's intelligent.

‚ú≥=[:h2 "The Null Hypothesis"] üîö

LLMs literally are designed to word sequence predictors. Because that is what we ‚ú≥=[:em "know"] üîö about their design, the null hypothesis should certainly be that they are exploiting statistical regularities in the text. Any claims of new capabilities should be taken with a lot of skepticism unless there is extremely well-established evidence, grounded in a rigorous internal understanding of how LLMs work, to believe otherwise. 

Unfortunately, the flawed-but-useful process of peer review has been thrown out the window in favor of a new style of scientific publishing: drop a "preprint" that may not have even been submitted anywhere on arxiv and let your Twitter followers engage in memetic warfare on behalf of the article's conclusions. Reply guy? I think you mean "peer reviewer," thank you very much.

Spooking yourself and your Twitter followers with scary stories about what the oracle told you about paperclip-maximizing robots may be emotionally compelling, but it isn't science.

