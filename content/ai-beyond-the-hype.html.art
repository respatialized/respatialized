<% (require '[hiccup.core :refer [html]]
             '[respatialized.render :refer :all]) %>


<%=(header "AI: Beyond The Hype") %>
<%=(header "Computation, labor, and power" :h2)%>

I'm going to talk today about machine learning and AI.

Supposedly we're just around the corner from a runaway breakthrough that changes our society as much as the invention of agriculture or the use of fossil fuels for motive power. I'm going to try and give you enough context for how this stuff works to persuade you of something you probably knew already: that fantasies of the impending obsolesence of human intelligence are Silicon Valley bullshit. But I also hope that I can convey <%(=(em "why")%> it's bullshit and demystify the actual technology so you can see through the hype in the future.

Who am I? I work in the field as a data engineer and previously as a data scientist. My role is to transform what researchers come up with into software that can be repeated and scaled up to deal with newer and larger sets of data.

<%=(header "Part 1: Fitting Lines")%>

The simplest form of machine learning you can do is figure out how to draw a straight line through a bunch of data points.

<%=(header "Fig. 1: Linear Regression" :h6)%>
<%=(print "OZ VISUALIZATION GOES HERE")%>

By drawing this line, we're trying to predict how much of a change in [CONCRETE THING] results from a change to [OTHER CONCRETE THING].

This plot is interactive, so I can mess with the terms that describe the line. Remember y = mx + b from high school algebra? We're changing m and b here. Just as with seeing it visually, it's pretty easy to change the terms enough to get to a "pretty good" solution quickly, but we don't know if it's the best solution - if it minimizes the error across all of its predictions.

Though our eyes visually infer the relationship between [CONCRETE THINGS] almost instantly, it's tougher to get a machine to do so, not least because computers don't do the kinds of approximation and guesswork that we do when we see this plot - they demand an exact answer and method!

A computer can find the line that minimizes the error by <%(em "repeatedly")%> going through every data point, measuring the distance between the line it draws and the data point, then adjusting and drawing a new line to better fit. So it's going through the data points multiple times to identify a better solution.

<%(em "TOTAL COMPUTATIONS: X_1")%>

With just two variables it's easy enough to try and find the fit line manually, but in the real world we have way more than two variables.

<%=(header "Fig. 2: Multivariate Regression" :h6)%>
<%=(print "OZ VISUALIZATION GRID GOES HERE")%>

Our dataset has 4 variables, meaning there are 16 possible combinations of them if we are comparing two at a time. Messing around with that visualization to find a good line for all of the plots in this grid sure sounds tedious. Instead we make the computer do it for us. (For the real heads, this is a <%(em "generalized linear model")%>)

So instead of repeatedly drawing one line 100 times, we're repeatedly drawing 16 lines 100 times each, each of which goes for N times, and so on. So we're making the computer do a lot more math pretty quickly.

<%(em "TOTAL COMPUTATIONS: X_2")%>

And all of this is just to fit models to <%(em "known")%> data. But the only reason businesses are interested in these methods is because they hope to get good solutions for <%(em "unknown")%> data.

<%=(header "Part 2: Training and Prediction" :h3)%>

When all you do is draw a line right through your observed data, you run the risk of essentially "memorizing" it.

<%=(header "Fig. 3: Overfitting" :h6)%>
<%=(print "OZ VISUALIZATION OF OLD/NEW DATA GOES HERE")%>

When we drew a line through the old data, we drew it too closely to those points, and so it did badly when we added new data to the mix. Effectively, we were trying to predict what we already knew. The line drawn by our model wasn't <%(em "general")%> enough to make good predictions.

Again, computers can help with this! We can simulate the difference between known and unknown data by deliberately excluding some of the data we have from the model when it tries to draw the line.

Once again, this is increasing the complexity of what we need to do to get good results. We're not just finding the line that minimizes the error between two data points: we're finding the line that minimizes the error between those data points across multiple randomly selected subsets of the data, so we have to do 10 or 100 versions of what we did above in order to get a good solution.

<%=(em "TOTAL COMPUTATIONS: X_3")%>

That's not a problem, because computers are fast, right? Well, maybe if your data are spreadsheet sized, you can do this in a few minutes or hours. But what if your data don't fit in a spreadsheet?

<%=(header "Part 3: Going Deep On Big Dataâ„¢" :h3)%>

Big data is what doesn't fit in a spreadsheet. Excel can only really handle a million rows, so beyond that you need to write some code to go through it.

So if we were trying to build a model that predicts from two million data points rather than 200, we'd be doing
<%=(em "TOTAL COMPUTATIONS: X_4")%>

Good luck running that on your laptop! Industry practicioners typically throw dozens, hundreds, or even thousands of virtual computers at these problems so they can obtain a result in a reasonable amount of time. Amazon Web Services, the leader in providing these virtual computers, is so profitable for Amazon that it lets them effectively run an at-cost ecommerce business while remaining extremely profitable as a company overall.

There are so many decisions about how to represent the features in the data, how to transform them or pre-process them, that it makes my head spin and I work in the field. So rather than try to figure it out ourselves, what if we leave <%=(em "all")%> the decisions about the structure of the data to the computer?

<%=(image "NEURAL NETWORK DIAGRAM")%>

This is what neural networks do. There are multiple layers in them, each of which fits a model to some sub-feature in the input data, and then passes what it has learned up to the next layer, which uses those predictions as inputs to its own predictions, and so on. It's basically an attempt to let the computer come up with its own patterns and correlations in the data.

Neural networks are surprisingly good at generating good predictions within a much shorter span of time than the "first generation" machine learning methods. What they're not as good as is explaining how their predictions work.

<%=(image "DEEP DREAM DOGS")%>
<%=(em "does this look like dogs to you?")%>